# Техническое задание на рефакторинг: Логика Chat Service

**Приоритет:** Критический (Critical)
**Суть проблемы:** Текущая реализация `ChatService` использует избыточные, последовательные вызовы LLM (`_should_start_search` -> `determine_query_type` -> `needs_visual_analysis`), что прямо противоречит архитектурным требованиям (Запрет на "if else на строках").
**Цель:** Заменить цепочку вызовов на **единый** вызов LLM с поддержкой инструментов (Tools/Function Calling).

---

## 1. Требование к архитектуре
Логика должна быть перестроена на агентную модель.
В состоянии `CHAT` мы делаем **только один запрос** к LLM. В этом запросе мы передаем:
1.  Системный промпт (Persona).
2.  Историю переписки.
3.  Описание доступных инструментов (в тексте системного промпта, формат Hermes/XML).

Модель **сама** решает, что ей сделать:
*   Просто ответить пользователю текстом.
*   Вызвать `quick_search`.
*   Инициировать переход к планированию `deep_research`.

## 2. Необходимые изменения в коде

### Удалить методы
Удалить следующие "узкие" методы из `ChatService` и `QueryProcessingService`:
*   ❌ `_should_start_search`
*   ❌ `determine_query_type`
*   ❌ `needs_visual_analysis`

### Изменить `process_user_message`
Метод должен выполнять один запрос `get_completion`.
В системный промпт (или отдельным сообщением `system`) нужно внедрить описание инструментов.

**Пример структуры Системного Промпта:**
```text
Ты — интеллектуальный агент по исследованию рынка на Avito.
Твоя задача — помогать пользователю искать товары, анализировать цены и характеристики.

У тебя есть доступ к следующим инструментам. Если ты решил выполнить поиск, ты ОБЯЗАН использовать формат XML:

<tools>
[
    {
        "name": "start_quick_search",
        "description": "Использовать для быстрого поиска конкретных товаров, когда запрос пользователя прост и понятен.",
        "parameters": {
            "query": "строка, поисковый запрос для Avito",
            "needs_visual": "bool, нужно ли анализировать изображения (одежда, дизайн) или достаточно текста (техника)"
        }
    },
    {
        "name": "initiate_deep_research_planning",
        "description": "Использовать, когда запрос пользователя сложный, размытый или требует сравнения множества характеристик. Переводит диалог в режим планирования схемы поиска.",
        "parameters": {
            "initial_topic": "тема исследования"
        }
    }
]
</tools>

Если ты вызываешь инструмент, оберни JSON вызова в тег <tool_call>.
Ты можешь совмещать обычный текст ответа и вызов инструмента.
Пример:
"Хорошо, я поищу для вас варианты. <tool_call>{ "name": "start_quick_search", ... }</tool_call>"
```

### Логика обработки ответа
После получения ответа от LLM (`response.content`), необходимо:
1.  Сохранить **весь** ответ (и текст, и теги) в историю чата (чтобы модель помнила, что она вызывала инструмент).
2.  С помощью регулярных выражений (regex) проверить наличие тега `<tool_call>...</tool_call>`.
3.  **Если тега нет:** просто вернуть ответ пользователю (это обычный разговор).
4.  **Если тег есть:**
    *   Распарсить JSON внутри тега.
    *   Если инструмент `start_quick_search` -> вызвать сервис поиска -> вернуть результат -> сменить стейт на `SEARCHING_QUICK`.
    *   Если инструмент `initiate_deep_research_planning` -> сменить стейт на `PLANNING_DEEP_RESEARCH` (далее логика согласования схемы).

## 3. Пример сценария (Flow)

**User:** "Привет, найди мне видеокарту 3060, только не майненную."

**Current (Wrong):**
1. LLM: "Это поиск?" -> YES.
2. LLM: "Это Deep или Quick?" -> QUICK.
3. LLM: "Нужны картинки?" -> NO.
4. Code: Запуск поиска. (Пользователь не получил текстового подтверждения).

**Target (Correct):**
1. **Input:** История + Промпт с инструментами.
2. **LLM Output:** "Конечно, сейчас гляну варианты 3060 на рынке. Постараюсь отфильтровать подозрительные варианты. <tool_call>{"name": "start_quick_search", "arguments": {"query": "rtx 3060 !майнинг", "needs_visual": false}}</tool_call>"
3. **System:**
   *   Показывает пользователю текст: "Конечно, сейчас гляну..."
   *   Парсит вызов.
   *   Запускает `QuickSearchService`.

## 4. Критерии приемки (DoD)
1.  В логах виден **ровно один** запрос к LLM при обработке сообщения пользователя (перед началом поиска).
2.  Ответ модели содержит как человеческий текст, так и `xml` блок (если нужен поиск).
3.  Параметры `needs_visual` и `query` заполняет сама LLM внутри одного вызова.